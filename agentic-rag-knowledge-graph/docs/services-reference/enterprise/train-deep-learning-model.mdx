---
title: "Train Deep Learning Model"
description: "API reference for the TrainDeepLearningModel task available in ArcGIS Enterprise."
slug: "train-deep-learning-model"
url: "/train-deep-learning-model"
guid: "GUID-47987FA6-3B09-46A6-9D01-0EAC78BA8060"
migratedTopicMetadata:
  FTITLE: "Train Deep Learning Model"
  FDESCRIPTION: ""
  FCHANGES: "per issue 14414 added section for 11.2"
  FISHRELEASELABEL: ""
  FESRISOFTWARERELEASE: ""
  FESRITAGCLOUD: ""
  FESRIRECREATIONPROCEDURE: ""
  FESRIPRODUCTLIFECYCLE: "11.2 - *"
  CREATED-ON: "12/10/2023 07:47:46"
  FAUTHOR: "jswain"
  FSTATUS: "Draft"
  MODIFIED-ON: "12/10/2023 08:24:22"
  VERSION: "9"
  FRESOLUTION: ""
  DOC-LANGUAGE: "en"
  FISHREVCOUNTER: "2"
  ED: "GUID-0E6DC7EB-0A4C-4B69-85D7-F7CCA738553B"
  FUSERGROUP: ""
  READ-ACCESS: ""
restInfoMetadata:
  apiPath: /TrainDeepLearningModel
  urlSegments:
    - name: <rasteranalysistools-url>
      url: /raster-analysis-tasks-overview
    - name: /TrainDeepLearningModel
  methods: []
  sslOnly: 'no'
  versionIntroduced: '10.8'
---

import GUID_187EFF3C_6544_4421_8D1E_9DFCF76354CD_GUID_C5A064C3_43B4_400B_AA66_7C41F2A060FB from "./snippets/GUID-187EFF3C-6544-4421-8D1E-9DFCF76354CD/_GUID-C5A064C3-43B4-400B-AA66-7C41F2A060FB";
import GUID_187EFF3C_6544_4421_8D1E_9DFCF76354CD_GUID_44A86E3D_176E_4BD4_A2E5_C112ED929DE9 from "./snippets/GUID-187EFF3C-6544-4421-8D1E-9DFCF76354CD/_GUID-44A86E3D-176E-4BD4-A2E5-C112ED929DE9";
import GUID_187EFF3C_6544_4421_8D1E_9DFCF76354CD_GUID_ED11CCC7_40D3_4ADD_A679_A3F6964834ED from "./snippets/GUID-187EFF3C-6544-4421-8D1E-9DFCF76354CD/_GUID-ED11CCC7-40D3-4ADD-A679-A3F6964834ED";
import GUID_92B97D46_70DD_4B77_80C3_E6A319680CD9_GUID_50D345C8_14F8_4A4A_AB1E_A6BBD19C49EF from "./snippets/GUID-92B97D46-70DD-4B77-80C3-E6A319680CD9/_GUID-50D345C8-14F8-4A4A-AB1E-A6BBD19C49EF";

## Description

![Train Deep Learning Model](./images/GUID-1AA5C60A-3F5F-478F-A72A-5BA54C973456-web.png)

The `TrainDeepLearningModel`  task is used to train a deep learning model using the output from the [`ExportTrainingDataforDeepLearning` ](/export-training-data-for-deep-learning) operation. It generates the deep learning model package (`*.dlpk` ) and adds it to an enterprise portal. You can also use this task to write the deep learning model package to a file share data store location.

<Note type={"note"}>
As of 10.5, you must license ArcGIS Server as an ArcGIS Image Server to use this resource.
</Note>
### 11.2

Cloud store and cloud raster store support was added for the `in_folder`  and `output_name`  parameters.

Portal items URLs are also supported as input for the `pretrained_model`  parameter.

##  Request parameters

<StyledTable headers={"Parameter,Details"}>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`in_folder` 

(Required)

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

This is the input location for the training sample data. It can be the path of the output location in the file share data store, file share raster store, cloud raster store, or a shared file system path. The training sample data folder must be the output from the [`ExportTrainingDataforDeepLearning` ](/export-training-data-for-deep-learning) operation, containing image and label folders, as well as the JSON model definition file written by the tool. 

The following are file share raster store path examples:

Examples
```javascript
//File share raster store path examples
in_folder=/fileShares/yourFileShareFolderName/trainingSampleData
in_folder={"uri":"/fileShares/yourfileShareFolderName/trainingSampleData"}

//File share path example
in_folder=/rasterStores/yourRasterStoreFolderName/trainingSampleData

//Cloud data store path example
in_folder=/cloudStores/yourCloudDatastoreName/trainingSampleData

//Cloud raster store path example
in_folder=/rasterStores/yourCloudRasterStoreName/trainingSampleData

//File share path example
in_folder=\\serverName\deepLearning\trainingSampleData

//Multiple input folders example
in_folder=/fileShares/yourFileShareFolderName/trainingSampleDataA,/fileShares/yourFileShareFolderName/trainingSampleDataB
in_folder={"uris":["/fileShares/yourFileShareFolderName/trainingSampleDataA","/fileShares/yourFileShareFolderName/trainingSampleDataB"]}
```

 

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`output_name` 

(Required)

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

This is the output location for the trained deep learning model package (`.dlpk` ). It can be a JSON object representing the output `.dlpk`  name that will be added as a portal item or a string of the folder path in the file share data store, file share raster store, cloud data store, or cloud raster store. The data store must be registered on the server.

Example:

```javascript
//Output dlpk name
output_name={"name": "trainedModel"}
output_name={"name": "trainedModel","folderId":"dfwerfbd3ec25584d0d8f4"}

//File share data store path
output_name=/fileShares/filesharename/folder

//Cloud data store path
output_name=/cloudStores/yourCloudStoreName/folder

//Raster store path
output_name=/rasterStores/yourFileShareRasterStoreName/folder
output_name=/rasterStores/yourCloudRasterStoreName/folder

//File share data store path:
output_name={"uri":"/fileShares/yourFileShareFolderName/trainedModel"}
```

 

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`model_type` 

(Required)

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The model type to use for training the deep learning model. This parameter supports model types for image translation, object classification, object detection, object tracker, and pixel classification. The model types that are supported for each type of processing and the supported values for this parameter are listed below.

Image translation values: `PIX2PIX`  \| `CYCLEGAN`  \| `SUPERRESOLUTION`  \| `PIX2PIXHD` 

Object classification values: `FEATURE_CLASSIFIER`  \| `IMAGECAPTIONER` 

Object detection values: `SSD`  \| `RETINANET`  \| `MASKRCNN`  \| `YOLOV3`  \| `FASTERRCNN`  \| `MMDETECTION`  \| `DETREG` 

Object tracker values: `SIAMMASK`  \| `DEEPSORT` 

Panoptic segmentation values: `MAXDEEPLAB` 

Pixel classification values: `UNET`  \| `PSPNET`  \| `DEEPLAB`  \| `BDCN_EDGEDETECTOR`  \| `HED_EDGEDETECTOR`  \| `MULTITASK_ROADEXTRACTOR`  \| `CONNECTNET`  \| `CHANGEDETECTOR`  \| `MMSEGMENTATION` 

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`arguments` 

(Optional)

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

This is where you list additional deep learning parameters and arguments for experiments and refinement, such as a confidence threshold for adjusting sensitivity. The names of the arguments are populated from reading the Python module.

When you set `model_type`  to `SSD` , the following arguments will be used:

-   `grids` —The number of grids the image will be divided into for processing. Setting this argument to 4 means the image will be divided into 4 x 4 or 16 grid cells. If no value is specified, the optimal grid value will be calculated based on the input imagery.
-   `zooms` —The number of zoom levels each grid cell will be scaled up or down. Setting this argument to 1 means all the grid cells will remain at the same size or zoom level. A zoom level of 2 means all the grid cells will become twice as large (zoomed in 100 percent). Providing a list of zoom levels means all the grid cells will be scaled using all the numbers in the list. The default is 1.0.
-   `ratios` —The list of aspect ratios to use for the anchor boxes. In object detection, an anchor box represents the ideal location, shape, and size of the object being predicted. Setting this argument to \[1.0,1.0], \[1.0, 0.5] means the anchor box is a square (1:1) or a rectangle in which the horizontal side is half the size of the vertical side (1:0.5). The default is \[1.0, 1.0].

When you set `model_type`  to any of the pixel classification models (`PSPNET` , `UNET` , or `DEEPLAB` ), the following arguments will be used:

-   `USE_UNET` —The U-Net decoder will be used to recover data once the pyramid pooling is complete. The default is True. This argument is specific to the PSPNET model.
-   `PYRAMID_SIZES` —The number and size of convolution layers to be applied to the different subregions. The default is \[1,2,3,6]. This argument is specific to the PSPNET model.
-   `MIXUP` —Specifies whether mixup augmentation and mixup loss will be used. The default is False.
-   `CLASS_BALANCING` —Specifies whether the cross-entropy loss inverse will be balanced to the frequency of pixels per class. The default is False.
-   `FOCAL_LOSS` —Specifies whether focal loss will be used. The default is False.
-   `IGNORE_CLASSES` —The list of class values on which the model will incur loss.

When you set `model_type`  to `RETINANET` , the following arguments will be used:

-   `SCALES` —The number of scale levels each cell will be scaled up or down. The default is \[1, 0.8, 0.63].
-   `RATIOS` —The aspect ratio of the anchor box. The default is \[0.5,1,2].
-   `MONITOR` —Specifies the metric to monitor when checkpointing and early stopping of training. Available metrics are `valid_loss` , `accuracy` , `miou` , and `dice` . The default is `valid_loss` .

All model types support the `chip_size`  argument, which is the chip size of the tiles in the training samples. The image chip size is extracted from the `.emd`  file in the input folder.

Syntax: The value pairs of arguments and their values.

Example

```javascript
arguments={"name1": "value1", "name2": "value2"}
```

 

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`batch_size` 

(Optional)

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The number of training samples to be processed for training at one time. If the server has a powerful GPU, this number can be increased to 16, 36, 64, and so on.

Example

```javascript
batch_size=4
```

 

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`max_epochs` 

(Optional)

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The maximum number of epochs for training the model. One epoch means the whole training dataset will be passed forward and backward through the deep neural network once.

Example

```javascript
max_epochs=20
```

 

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`learning_rate` 

(Optional)

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The rate at which the weights are updated during the training. It is a small positive value in the range between 0.0 and 1.0. If the learning rate is set to 0, it will extract the optimal learning rate from the learning curve during the training process.

Example

```javascript
learning_rate=0
```

 

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`backbone_model` 

(Optional)

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

Specifies the preconfigured neural network to be used as an architecture for training the new model. See the [Backbone model values](/train-deep-learning-model#GUID-DFADB6C2-77C2-4D61-B8C7-131C9FD2BF51) section below for more information.

Values: `DARKNET53`  \| `DENSENET121`  \| `DENSENET161`  \| `DENSENET169`  \| `DENSENET201`  \| `MOBILENET_V2` \| `REID_V1`  \| `REID_V2`  \| `RESNET18`  \| `RESNET34`  \| `RESNET50`  \| `RESNET101`  \| `RESNET152`  \| `VGG11`  \| `VGG11_BN`  \| `VGG13`  \| `VGG13_BN`  \| `VGG16`  \| `VGG16_BN`  \| `VGG19`  \|`VGG19_BN` 

Example

```javascript
backbone_model=RESNET34
```

 

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`validation_percent` 

(Optional)

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The percentage of training sample data that will be used for validating the model.

Example

```javascript
validation_percent=10
```

 

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`pretrained_model` 

(Optional)

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The pretrained model to be used for fine-tuning the new model. It is a `.dlpk`  portal item.

Example

```javascript
pretrained_model={"itemId": "8cfbd3ec25584d0d8fed23b8ff7c43b"}
pretrained_model={"url":"https://www.arcgis.com/sharing/rest/content/items/916e02960d9e495baeb4d1d2ff4055d0"}
```

 

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`stop_training` 

(Optional)

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

Specifies whether early stopping will be implemented. If `true` , the model training will stop when the model is no longer improving, regardless of the maximum epochs specified. This is the default. If `false` , the model training will continue until the maximum epochs is reached.

Values: `true`  \| `false` 

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`overwriteModel` 

(Optional)

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

Overwrites an existing deep learning model package (`.dlpk` ) portal item with the same name.

If the `output_name`  parameter uses the file share data store path, the `overwriteModel`  parameter is not applied.

-   `True` —The portal `.dlpk`  item will be overwritten.
-   `False` —The portal `.dlpk`  item will not be overwritten. This is the default.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`context` 

(Optional)

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

Environment settings that affect task operation. This parameter has the following settings:

-   [extent](/extent-ra) —A bounding box that defines the analysis area.
-   [cellSize](/snap-raster) —The output raster will have the resolution specified by cell size.
-   [processorType](/processor-type) —The specified processor (CPU or GPU) will be used for the analysis.
-   [parallelProcessingFactor](/parallel-processing-factor) —The number of logical processes across which a tool will operate.

Example

```javascript
context={"cellSize": "20","processorType": "GPU"}
```

 

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`freeze_Model` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

Specifies whether the backbone layers in the pretrained model will be frozen so that the weights and biases in the backbone layers remain unaltered. If `true` , the predefined weights and biases will not be altered in the `backboneModel`  value. This is the default. If `false` , the weights and biases of the `backboneModel`  value may be altered to better fit the training samples. This may take more time to process but typically produces better results.

Values: `true`  \| `false` 

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`f` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The response format. The default response format is `html` .

Values: `html`  \| `json`  \| `pjson` 

</StyledTableCell>
</StyledTableRow>
</StyledTable>

## Backbone model values

The accepted preconfigured neural network values that can be submitted with the `backbone_model`  parameter are described below.

<StyledTable headers={"Value,Description"}>

<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`DARKNET53` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The preconfigured model will be a convolutional neural network trained on the ImageNet dataset that contains more than 1 million images and is 53 layers deep.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`DENSENET121` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The preconfigured model will be a dense network trained on the ImageNet dataset that contains more than 1 million images and is 121 layers deep. Unlike RESNET, which combines the layer using summation, DenseNet combines the layers using concatenation.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`DENSENET161` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The preconfigured model will be a dense network trained on the ImageNet dataset that contains more than 1 million images and is 161 layers deep. Unlike RESNET, which combines the layer using summation, DenseNet combines the layers using concatenation.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`DENSENET169` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The preconfigured model will be a dense network trained on the ImageNet dataset that contains more than 1 million images and is 169 layers deep. Unlike RESNET, which combines the layer using summation, DenseNet combines the layers using concatenation.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`DENSENET201` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The preconfigured model will be a dense network trained on the ImageNet dataset that contains more than 1 million images and is 201 layers deep. Unlike RESNET, which combines the layer using summation, DenseNet combines the layers using concatenation.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`MOBILENET_V2` 

</StyledTableCell>
 <StyledTableCell cellStyle="markdown">

The preconfigured model trained on the ImageNet database and is 54 layers deep geared toward Edge device computing, since it uses less memory.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`RESNET18` 

</StyledTableCell>
 <StyledTableCell cellStyle="markdown">

The preconfigured model will be a residual network trained on the ImageNet dataset that contains more than 1 million images and is 18 layers deep.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`RESNET34` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The preconfigured model will be a residual network trained on the ImageNet dataset that contains more than 1 million images and is 34 layers deep. This is the default.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`RESNET50` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The preconfigured model will be a residual network trained on the ImageNet dataset that contains more than 1 million images and is 50 layers deep.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`RESNET101` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The preconfigured model will be a residual network trained on the ImageNet dataset that contains more than 1 million images and is 101 layers deep.

</StyledTableCell>

</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`RESNET152` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The preconfigured model will be a residual network trained on the ImageNet dataset that contains more than 1 million images and is 152 layers deep.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`VGG11` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The preconfigured model will be a convolution neural network trained on the ImageNet dataset that contains more than 1 million images to classify images into 1,000 object categories and is 11 layers deep.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>

<StyledTableCell cellStyle="markdown">

`VGG11_BN` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The preconfigured model is based on the VGG network but with batch normalization, which normalizes each layer in the network. It trained on the ImageNet dataset and has 11 layers.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`VGG13` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The preconfigured model will be a convolution neural network trained on the ImageNet dataset that contains more than 1 million images to classify images into 1,000 object categories and is 13 layers deep.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`VGG13_BN` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The preconfigured model is based on the VGG network but with batch normalization, which normalizes each layer in the network. It trained on the ImageNet dataset and has 13 layers.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`VGG16` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The preconfigured model will be a convolution neural network trained on the ImageNet dataset that contains more than 1 million images to classify images into 1,000 object categories and is 16 layers deep.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`VGG16_BN` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The preconfigured model is based on the VGG network but with batch normalization, which normalizes each layer in the network. It trained on the ImageNet dataset and has 16 layers.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`VGG19` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The preconfigured model will be a convolution neural network trained on the ImageNet dataset that contains more than 1 million images to classify images into 1,000 object categories and is 19 layers deep.

</StyledTableCell>
</StyledTableRow>
<StyledTableRow>
<StyledTableCell cellStyle="markdown">

`VGG19_BN` 

</StyledTableCell>
<StyledTableCell cellStyle="markdown">

The preconfigured model is based on the VGG network but with batch normalization, which normalizes each layer in the network. It trained on the ImageNet dataset and has 19 layers.

</StyledTableCell>
</StyledTableRow>
</StyledTable>

## Example usage

The following is a sample request URL for `TrainDeepLearningModel` :

```javascript
https://services.myserver.com/arcgis/rest/services/System/RasterAnalysisTools/GPServer/TrainDeepLearningModel
```

 

## Response

<GUID_187EFF3C_6544_4421_8D1E_9DFCF76354CD_GUID_C5A064C3_43B4_400B_AA66_7C41F2A060FB />

<GUID_187EFF3C_6544_4421_8D1E_9DFCF76354CD_GUID_44A86E3D_176E_4BD4_A2E5_C112ED929DE9 />

<GUID_187EFF3C_6544_4421_8D1E_9DFCF76354CD_GUID_ED11CCC7_40D3_4ADD_A679_A3F6964834ED />

```javascript
https://<rasterAnalysisTools-url>/TrainDeepLearningModel/jobs/<jobId>
```

 

<GUID_92B97D46_70DD_4B77_80C3_E6A319680CD9_GUID_50D345C8_14F8_4A4A_AB1E_A6BBD19C49EF />

```javascript
https://<rasterAnalysisTools-url>/TrainDeepLearningModel/jobs/<jobId>/results/out_item
```

 

## JSON Response example

The response returns the `.dlpk`  portal item, which has `title` , `type` , `filename` , `file` , `id` , and `folderId`  properties.

```other
{
  "title": "dlpk_name",
  "type": "Deep Learning Package",
  "multipart": True,
  "tags": "imagery" 
  "typeKeywords": "Deep Learning, Raster"
  "filename": "dlpk_name",
  "file": "\\servername\rasterstore\mytrainedmodel.dlpk",
  "id": "f121390b85ef419790479fc75b493efd",
  "folderId": "dfwerfbd3ec25584d0d8f4"
}
```

 

However, if a data store path is specified as the value for `output_name` , the output will be the data store location.

```javascript
{
  "paramName": "out_item",
  "dataType": "GPString",
  "value": {"uri": "/fileShares/yourFileShareFolderName/trainedModel/trainedModel.dlpk"}"value": {"uri": "/fileShares/yourFileShareFolderName/trainedModel/trainedModel.dlpk"}
}
```

 
